%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{About the new coefficient of correlation paper}

\section{Problems with correlation coefficients}
\begin{enumerate}
    \item The three most popular classical measures of statistical association are Pearson's correlation coefficient, Spearman's $\rho$, and Kendall's $\tau$.
    \item They are very good at detecting linear or monotone relations between variables, and  have well developed statistical theories. But they fail to detect non-monotone associations (e.g. periodic relations).
    \item Many coefficients have been proposed in the past to address these issues. But most of them have these two problems.
    % Add some proposed correlation coefficients later
    
    \begin{enumerate}
        \item One would like the coefficient to be close to its maximum value iff one variable is close to being a noiseless function of the other. For example in Pearson coefficient, it is 1 iff one variable is close to being a linear function of the other. They are 1 when it one variable is a noiseless function of the other but the converse doesn't hold.
        \item They do not have simple asymptotic theories under the hypothesis of independence that facilitate the quick computation of p-values for testing independence. We have to rely on expensive permutation tests or bootstrap to test for independence.
    \end{enumerate}
    \item The Chatterjee correlation coefficient (not the official name) presented in the next section addresses the above two issues.
\end{enumerate}    

\section{Chatterjee Correlation Coefficient}
This coefficient is (a) as simple as the classical ones, (b) is a consistent estimator of some measure of dependence which is 0 iff the variables are independent, and 1 iff one is a measurable function of the other, and (c) has a simple asymptotic theory under the hypothesis of independence, like the classical coefficients. \\\\
Let $(X, Y)$ be a pair of random variables, where Y is not a constant (for our purposes, both X and Y are continuous). Let $\{(X_i, Y_i)\}_{i = 1}^{n}$ be i.i.d. pairs following the same distribution as $(X, Y)$.
\begin{enumerate}
    \item The case when $X_i's \text{ and } Y_i's$ have no ties. Rearrange the data as $(X_{(1)}, Y_{(1)}), \dots, (X_{(n)}, Y_{(n)})$ such that $X_{(1)} < \dots < X_{(n)}$. Let $r_i$ be the rank of $Y_{(i)}$, i.e. the number of $j$ such that $Y_{(j)} \leq Y_{(i)}$.  Then the correlation coefficient $\xi_n$ is defined to be
    $$\xi_n(X, Y) := 1-\frac{3\sum_{i=1}^{n-1} |r_{i+1} - r_i|}{n^2-1}$$.
    
    \item In the case of ties. If there are ties in $X_i's$, choose an increasing arrangement as follows and break ties uniformly at random. Let $r_i$ defined as above, and define $l_i$ to be the number of $j$ such that $Y_{(j)} \geq Y_{(i)}$. Define 
    $$\xi_n(X, Y) := 1-\frac{n\sum_{i=1}^{n-1} |r_{i+1} - r_i|}{2\sum_{i=1}^{n-1}l_i(n-l_i)}$$.
    When there are no ties among the $Y_i's, l_1, \dots, l_n$ is just a permutation of $1, \dots, n$ and the denominator is just $n(n^2-1)/3$, which reduces to the definition in the no ties case.
\end{enumerate}

\begin{theorem}
If $Y$ is not almost surely a constant, then as $n \rightarrow \infty$, $\xi_n(X, Y)$ converges almost surely to the deterministic limit 
$$\xi(X, Y) := \frac{\int Var(\mathbb{E}(1_{\{Y \geq t\}}|X)) d\mu(t)}{\int Var(1_{\{Y \geq t\}}) d\mu(t)}$$
where $\mu$ is the pdf of $Y$. This limit belongs to the interval $[0, 1]$. It is 0 iff X and Y are independent, and it is 1 iff there is a measurable function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that $Y = f(X)$ almost surely.
\end{theorem}
\subsection{Questions relevant to our purpose and some TODOs}
\begin{enumerate}
    \item Develop an intuitive understanding of Theorem 1.1 [Read the proof given in Section 8 and [10] of the main paper].
    \item How good is the estimator $\xi_n(X, Y)$ if our data is not i.i.d.? [Simulate some basic examples. Do for normal distribution (both for i.i.d. and MCMC data)]
    \item If $X_1, \dots, X_n$ is a MC, is $\xi(X_k, X_{k+j})$ independent of $k$ as seen in the Pearson correlation, i.e. same for all $k$? 
    \item If $X_1, \dots, X_n$ is a MC, $\xi(X_k, X_{k+j}) \rightarrow 0$ as $j \rightarrow \infty$? It seems true intuitively as as $\xi(X, Y) = 0$ iff $X, Y$ are independent, and in this case, as $X_k, X_{k+j}$ are independent as $j \rightarrow \infty$ [Prove that the independence happens in the limiting case and done!]
    \item For the above MC, is $\xi(X_k, X_{k+j})$ similar to $Corr(X_k, X_{k+j})$ for similar $j$? [Simulate some basic examples, AR(1), AR(2)]
\end{enumerate}

\subsection{Some remarks till now}
\begin{enumerate}
    \item $\xi_n$ is not symmetric in $X, Y$. This is intentional and useful as we might want to study if $Y$ is a measurable function of $X$, or $X$ is a measurable function of $Y$. \\ To get a symmetric coefficient, it suffices to consider $max(\xi_n(X, Y), \xi_n(Y, X))$. This estimator by Theorem 1.1 will converge to the maximum of respective limits. This will be equal to 0 iff $X, Y$ are independent, and 1 iff $X$ is a measurable function of $Y$ or $Y$ is a measurable function of $X$.
    
    \item $\xi(X, Y) \in [0, 1]$ \\ 
    $\textbf{Proof:}$ We know that for any random variables X and Y, 
    $$Var(X) = Var(\mathbb{E}(X|Y)) + \mathbb{E}(Var(X|Y))$$
    $$\implies Var(1_{\{Y \geq t\}}) \geq Var(\mathbb{E}(1_{\{Y \geq t\}}|X))$$
    Hence the integral will always be in [0, 1].
    
    \item If X and Y are independent, then $\mathbb{E}(1_{\{Y \geq t\}})$ is a constant, and so $\xi(X, Y) = 0$ \\ If X is a measurable function of Y, then $\mathbb{E}(1_{\{Y \geq t\}}|X) = 1_{\{Y \geq t\}}$, and so $\xi(X, Y) = 1$.
    % Ask doubt about the point above
    
    \item If there are no ties among $Y_i's$, then the maximum possible value of $\xi_n(X, Y)$ is $\frac{n-2}{n+1}$, attained when $X_i = Y_i$ for all $i$. This is very small for small n. Say for $n=20$, maximum value of $\xi_n(X,Y)$ is 0.86. 
    
    \item On the other hand, the smallest value of $\xi_n(X, Y)$ is $-1/2 + O(1/n)$, attained when top $n/2$ values of $Y_i$ are placed alternately with the bottom $n/2$ values. This is strange as it suggests that the value can go way below 0, but theorem 1.1 states that in the limiting case, it is non-negative. The only interpretation of a negative value is that the data used for estimation is correlated (Our MCMC case).
    % Need to test this by simulations
    
\end{enumerate}


