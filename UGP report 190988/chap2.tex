\chapter{Some new results related to the Chatterjee correlation and Markov chains}

Let $X_1, X_2, \dots$ be a stationary, time homogeneous Markov chain with distribution $\pi$.
\begin{theorem}
    $Cov(X_k, X_{k+t})$ is independent of $k$.
    \begin{proof}
        We know that
        $$Cov(X_k, X_{k+t}) = \mathbb{E}[X_k X_{k+t}] - \mathbb{E}[X_k]\mathbb{E}[X_{k+t}]$$
        Also, as $X_n$ is a stationary markov chain, $\mathbb{E}[X_n] = \mu$, where $\mu$ is the mean of the distribution $\pi$.
        And,
        $$\mathbb{E}[X_k X_{k+t}] = \int\int xyf_{(X_k, X_{k+t})}(x, y)dxdy$$
        where $f_{(X_k, X_{k+t})}$ is the joint density of $X_k$ and $X_{k+t}$. Now as the markov chain is stationary, this density is dependent only on $t$, i.e. $$f_{(X_k, X_{k+t})} = f_{(X_1, X_{1+t})}$$
        As both the terms of $Cov(X_k, X_{k+t})$ are independent of $k$, $Cov(X_k, X_{k+t})$ is independent of $k$.\\
    \end{proof}
\end{theorem}

\begin{theorem}
    $\xi(X_n, X_{n+k})$ is independent of $n$.
    \begin{proof}
        \begin{equation}
            \xi_{(X_n, X_{n+k})} = \frac{\int \Var{\mathbb{E}[1_{\{X_{n+k} \geq t\}}|X_n=x]} d\pi(t)}{\int \Var{1_{\{X_{n+k} \geq t\}}} d\pi(t)}
        \end{equation}
        We'll prove that both the numerator and the denominator are independent of $k$. \\
        We can write
        \begin{equation*}
            \mathbb{E}[1_{\{X_{n+k} \geq t\}}|X_n=x] = \Pr(X_{n+k} \geq t|X_n=x) \\
        \end{equation*}
        and by time-homogeneity of our Markov chain
        \begin{equation*}
            \Pr(X_{n+k} \geq t|X_n=x) = \int_t^{\infty} P^k(x, dy)
        \end{equation*}
        which is independent of $n$. And hence,
        \begin{equation}
            \int \Var{\int_t^{\infty} P^k(x, dy)} d\pi(u)
        \end{equation}
        is also independent of $n$.\\\\
        Now for the denominator, we know by stationarity of our Markov chain that $X_n \sim \pi$, so for any function $f, f(X_n) \sim \pi'$ for some distribution $\pi'$, and therefore the variance will be same for all $n$.\\
        Let $f_t: \mathbb{R} \rightarrow \mathbb{R}$ such that $f_t(X) = 1_{\{X \geq t\}}$.\\
        We can write the denominator as
        $$\int \Var{f_t(X_{n+k})} d\pi(t)$$
        where,
        $$\Var{f_t(X_{n+k})} = \Var{f_t(X_{1})}$$
        Therefore,
        $$\int \Var{f_t(X_{n+k})} d\pi(t)$$
        is independent of both $n$ and $k$.\\\\
        As both the numerator and denominator are independent of $n$, we can conclude that $\xi(X_n, X_{n+k})$ is independent of $n$.
    \end{proof}
\end{theorem}

\begin{theorem}
    $\xi(X_n, X_{n+k}) = \xi(X_{n+k}, X_n)$ for time reversal Markov chains.
    \begin{proof}
        From (2), we know that the denominator of $\xi(X_n, X_{n+k})$ is independent of both $n$ and $k$. So we only have to prove that the numerator is symmetric. \\
        We have to show that
        \begin{equation*}
            \int \Var {\Pr (X_{n+k} \geq t | X_n)} d\pi(t) = \int \Var {\Pr (X_n \geq t | X_{n+k})} d\pi(t)
        \end{equation*}
        \begin{lem}
            For a time reversible Markov chain, $X_n$ and $X_{n+k}$ are exchangable, i.e.
            \begin{equation*}
                f_{(X_{n}, X_{n+k})}(x, y) = f_{(X_{n+k}, X_{n})}(x, y) \text{  } \forall (x, y) \in \mathbb{R}^2
            \end{equation*}
            \begin{proof}
                Will write it later.
            \end{proof}
        \end{lem}
        By this lemma, it is clear that
        \begin{equation*}
            \Pr (X_{n+k} \geq t | X_n) = \Pr (X_n \geq t | X_{n+k}) \text{  } \forall t \in \mathbb{R}
        \end{equation*}
        which implies the result above.
    \end{proof}
\end{theorem}
    \newpage

\begin{theorem}
    $\lim_{n \rightarrow \infty} \xi(X_1, X_{n}) = 0$ for an Ergodic Markov chain
    \begin{proof}
        We have
        \begin{equation*}
            \xi(X_1, X_{n}) = \frac{\int \Var{\mathbb{E}[1_{\{X_{n} \geq t\}}|X_1=x]} d\pi(t)}{\int \Var{1_{\{X_{n} \geq t\}}} d\pi(t)}
        \end{equation*}
        The denominator is independent of $n$ as proven in (2), so we only need to show that the numerator goes to 0 as $n \rightarrow \infty$.\\
        \begin{lem}
            $$\lim_{n \rightarrow \infty} \int \Var{\mathbb{E}[1_{\{X_{n} \geq t\}}|X_1=x]} d\pi(t) = \int \lim_{n \rightarrow \infty}\Var{\mathbb{E}[1_{\{X_{n} \geq t\}}|X_1=x]} d\pi(t)$$
            \begin{proof}
                Prove it later
            \end{proof}
        \end{lem}
        \begin{lem}
            $$\lim_{n \rightarrow \infty} \Var{\mathbb{E}[1_{\{X_{n} \geq t\}}|X_1=x]} = \Var{\lim_{n \rightarrow \infty} \mathbb{E}[1_{\{X_{n} \geq t\}}|X_1=x]}$$
            \begin{proof}
                Prove it later
            \end{proof}
        \end{lem}
        Now, by (2.4), we know that
        \begin{equation*}
            \mathbb{E}[1_{\{X_{n} \geq t\}}|X_1=x] = \int_t^{\infty} P^{n-1}(x, dy)
        \end{equation*}
        \begin{lem}
            $$\lim_{n \rightarrow \infty} \int_t^{\infty} P^{n}(x, dy) = \int_t^{\infty} \lim_{n \rightarrow \infty} P^{n}(x, dy)$$\
            \begin{proof}
                Prove it later
            \end{proof}
        \end{lem}
        For an Ergodic Markov chain, under the Total Variation Norm, we know that
        \begin{equation*}
            ||P^k(x, \cdot) - F(\cdot)|| \rightarrow 0 \text{ as } k \rightarrow \infty
        \end{equation*}
        This implies
        \begin{align*}
            \lim_{n \rightarrow \infty} \int \Var{\mathbb{E}[1_{\{X_{n} \geq t\}}|X_1=x]} d\pi(t) &= \lim_{n \rightarrow \infty} \int \Var{\int_t^{\infty} P^{n-1}(x, dy)} d\pi(t)\\
            &= \int \Var{\int_t^{\infty} \lim_{n \rightarrow \infty} P^{n-1}(x, dy)} d\pi(t)\\
            &= \int \Var{\int_t^{\infty} F(dy)} d\pi(t)\\
            &= \int \Var{1-F(t)} d\pi(t)\\
            &= \int 0 \cdot d\pi(t)\\
            &= 0
        \end{align*}
        under the Total Variation Norm.
    \end{proof}
\end{theorem}
